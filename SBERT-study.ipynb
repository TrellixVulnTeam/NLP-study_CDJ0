{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# #Our sentences we like to encode\n",
    "# sentences = ['This framework generates embeddings for each input sentence',\n",
    "#     'Sentences are passed as a list of string.',\n",
    "#     'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# #Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding[:10])\n",
    "    \n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Sentence Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\")\n",
    "emb2 = model.encode(\"Have you seen my red cat?\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models\n",
    "https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Cross-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "\n",
    "#For Example\n",
    "scores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base', device='cuda')\n",
    "sources = [('A man is eating pizza.', 'A man eats something.'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')]\n",
    "scores = model.predict(sources)\n",
    "print(scores)\n",
    "\n",
    "#Convert scores to labels\n",
    "label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
    "for i, (a, b) in enumerate(sources):\n",
    "    print(a, b, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings with Transformers\n",
    "\n",
    "Most of our pre-trained models are based on Huggingface.co/Transformers and are also hosted in the models repository from Huggingface. It is possible to use our sentence embeddings models without installing sentence-transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    hits = hits[0]      #Get the hits for the first query\n",
    "    for hit in hits:\n",
    "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve & Re-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also compare the results to lexical search (keyword search). Here, we use \n",
    "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Sematic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(query = \"What is the capital of South Korea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(query = \"Number countries Europe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: clustering\n",
    "Sentences are mapped to sentence embeddings and then k-mean clustering is applied.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'A man is eating pasta.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'The baby is carried by the woman',\n",
    "          'A man is riding a horse.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.',\n",
    "          'A cheetah is running behind its prey.',\n",
    "          'A cheetah chases prey on across a field.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "\n",
    "# Perform kmean clustering\n",
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single list of sentences - Possible tens of thousands of sentences\n",
    "sentences = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'I love pasta',\n",
    "             'The new movie is awesome',\n",
    "             'The cat plays in the garden',\n",
    "             'A woman watches TV',\n",
    "             'The new movie is so great',\n",
    "             'Do you like pizza?']\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translated Sentence Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitext_mining_utils\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "\n",
    "def score(x, y, fwd_mean, bwd_mean, margin):\n",
    "    return margin(x.dot(y), (fwd_mean + bwd_mean) / 2)\n",
    "\n",
    "\n",
    "def score_candidates(x, y, candidate_inds, fwd_mean, bwd_mean, margin):\n",
    "    scores = np.zeros(candidate_inds.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(scores.shape[1]):\n",
    "            k = candidate_inds[i, j]\n",
    "            scores[i, j] = score(x[i], y[k], fwd_mean[i], bwd_mean[k], margin)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def kNN(x, y, k, use_ann_search=False, ann_num_clusters=32768, ann_num_cluster_probe=3):\n",
    "    start_time = time.time()\n",
    "    if use_ann_search:\n",
    "        print(\"Perform approx. kNN search\")\n",
    "        n_cluster = min(ann_num_clusters, int(y.shape[0]/1000))\n",
    "        quantizer = faiss.IndexFlatIP(y.shape[1])\n",
    "        index = faiss.IndexIVFFlat(quantizer, y.shape[1], n_cluster, faiss.METRIC_INNER_PRODUCT)\n",
    "        index.nprobe = ann_num_cluster_probe\n",
    "        index.train(y)\n",
    "        index.add(y)\n",
    "        sim, ind = index.search(x, k)\n",
    "    else:\n",
    "        print(\"Perform exact search\")\n",
    "        idx = faiss.IndexFlatIP(y.shape[1])\n",
    "        idx.add(y)\n",
    "        sim, ind = idx.search(x, k)\n",
    "\n",
    "    print(\"Done: {:.2f} sec\".format(time.time()-start_time))\n",
    "    return sim, ind\n",
    "\n",
    "\n",
    "def file_open(filepath):\n",
    "    #Function to allowing opening files based on file extension\n",
    "    if filepath.endswith('.gz'):\n",
    "        return gzip.open(filepath, 'rt', encoding='utf8')\n",
    "    elif filepath.endswith('xz'):\n",
    "        return lzma.open(filepath, 'rt', encoding='utf8')\n",
    "    else:\n",
    "        return open(filepath, 'r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This scripts show how to mine parallel (translated) sentences from two list of monolingual sentences.\n",
    "As input, you specific two text files that have sentences in every line. Then, the\n",
    "LaBSE model is used to find parallel (translated) across these two files.\n",
    "The result is written to disc.\n",
    "A large source for monolingual sentences in different languages is:\n",
    "http://data.statmt.org/cc-100/\n",
    "This script requires that you have FAISS installed:\n",
    "https://github.com/facebookresearch/faiss\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import numpy as np\n",
    "# from sentence_transformers.bitext_mining_utils import *\n",
    "import gzip\n",
    "import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "#Model we want to use for bitext mining. LaBSE achieves state-of-the-art performance\n",
    "model_name = 'LaBSE'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "#Input files. We interpret every line as sentence.\n",
    "source_file = \"data/so.txt.xz\"\n",
    "target_file = \"data/yi.txt.xz\"\n",
    "\n",
    "# Only consider sentences that are between min_sent_len and max_sent_len characters long\n",
    "min_sent_len = 10\n",
    "max_sent_len = 200\n",
    "\n",
    "# We base the scoring on k nearest neighbors for each element\n",
    "knn_neighbors = 4\n",
    "\n",
    "# Min score for text pairs. Note, score can be larger than 1\n",
    "min_threshold = 1\n",
    "\n",
    "#Do we want to use exact search of approximate nearest neighbor search (ANN)\n",
    "#Exact search: Slower, but we don't miss any parallel sentences\n",
    "#ANN: Faster, but the recall will be lower\n",
    "use_ann_search = True\n",
    "\n",
    "#Number of clusters for ANN. Each cluster should have at least 10k entries\n",
    "ann_num_clusters = 32768\n",
    "\n",
    "#How many cluster to explorer for search. Higher number = better recall, slower\n",
    "ann_num_cluster_probe = 3\n",
    "\n",
    "#To save memory, we can use PCA to reduce the dimensionality from 768 to for example 128 dimensions\n",
    "#The encoded embeddings will hence require 6 times less memory. However, we observe a small drop in performance.\n",
    "use_pca = True\n",
    "pca_dimensions = 128\n",
    "\n",
    "\n",
    "if use_pca:\n",
    "    # We use a smaller number of training sentences to learn the PCA\n",
    "    train_sent = []\n",
    "    num_train_sent = 20000\n",
    "\n",
    "    with file_open(source_file) as fSource, file_open(target_file) as fTarget:\n",
    "        for line_source, line_target in zip(fSource, fTarget):\n",
    "            if min_sent_len <= len(line_source.strip()) <= max_sent_len:\n",
    "                sentence = line_source.strip()\n",
    "                train_sent.append(sentence)\n",
    "\n",
    "            if min_sent_len <= len(line_target.strip()) <= max_sent_len:\n",
    "                sentence = line_target.strip()\n",
    "                train_sent.append(sentence)\n",
    "\n",
    "            if len(train_sent) >= num_train_sent:\n",
    "                break\n",
    "\n",
    "    print(\"Encode training embeddings for PCA\")\n",
    "    train_matrix = model.encode(train_sent, show_progress_bar=True, convert_to_numpy=True)\n",
    "    pca = PCA(n_components=pca_dimensions)\n",
    "    pca.fit(train_matrix)\n",
    "\n",
    "    dense = models.Dense(in_features=model.get_sentence_embedding_dimension(), out_features=pca_dimensions, bias=False, activation_function=torch.nn.Identity())\n",
    "    dense.linear.weight = torch.nn.Parameter(torch.tensor(pca.components_))\n",
    "    model.add_module('dense', dense)\n",
    "\n",
    "\n",
    "print(\"Read source file\")\n",
    "source_sentences = set()\n",
    "with file_open(source_file) as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        line = line.strip()\n",
    "        if len(line) >= min_sent_len and len(line) <= max_sent_len:\n",
    "            source_sentences.add(line)\n",
    "\n",
    "print(\"Read target file\")\n",
    "target_sentences = set()\n",
    "with file_open(target_file) as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        line = line.strip()\n",
    "        if len(line) >= min_sent_len and len(line) <= max_sent_len:\n",
    "            target_sentences.add(line)\n",
    "\n",
    "print(\"Source Sentences:\", len(source_sentences))\n",
    "print(\"Target Sentences:\", len(target_sentences))\n",
    "\n",
    "\n",
    "### Encode source sentences\n",
    "source_sentences = list(source_sentences)\n",
    "\n",
    "\n",
    "print(\"Encode source sentences\")\n",
    "source_embeddings = model.encode(source_sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "### Encode target sentences\n",
    "target_sentences = list(target_sentences)\n",
    "\n",
    "print(\"Encode target sentences\")\n",
    "target_embeddings = model.encode(target_sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "# Normalize embeddings\n",
    "x = source_embeddings\n",
    "x = x / np.linalg.norm(x, axis=1, keepdims=True)\n",
    "\n",
    "y = target_embeddings\n",
    "y = y / np.linalg.norm(y, axis=1, keepdims=True)\n",
    "\n",
    "# Perform kNN in both directions\n",
    "x2y_sim, x2y_ind = kNN(x, y, knn_neighbors, use_ann_search, ann_num_clusters, ann_num_cluster_probe)\n",
    "x2y_mean = x2y_sim.mean(axis=1)\n",
    "\n",
    "y2x_sim, y2x_ind = kNN(y, x, knn_neighbors, use_ann_search, ann_num_clusters, ann_num_cluster_probe)\n",
    "y2x_mean = y2x_sim.mean(axis=1)\n",
    "\n",
    "# Compute forward and backward scores\n",
    "margin = lambda a, b: a / b\n",
    "fwd_scores = score_candidates(x, y, x2y_ind, x2y_mean, y2x_mean, margin)\n",
    "bwd_scores = score_candidates(y, x, y2x_ind, y2x_mean, x2y_mean, margin)\n",
    "fwd_best = x2y_ind[np.arange(x.shape[0]), fwd_scores.argmax(axis=1)]\n",
    "bwd_best = y2x_ind[np.arange(y.shape[0]), bwd_scores.argmax(axis=1)]\n",
    "\n",
    "indices = np.stack([np.concatenate([np.arange(x.shape[0]), bwd_best]), np.concatenate([fwd_best, np.arange(y.shape[0])])], axis=1)\n",
    "scores = np.concatenate([fwd_scores.max(axis=1), bwd_scores.max(axis=1)])\n",
    "seen_src, seen_trg = set(), set()\n",
    "\n",
    "#Extact list of parallel sentences\n",
    "print(\"Write sentences to disc\")\n",
    "sentences_written = 0\n",
    "with gzip.open('parallel-sentences-out.tsv.gz', 'wt', encoding='utf8') as fOut:\n",
    "    for i in np.argsort(-scores):\n",
    "        src_ind, trg_ind = indices[i]\n",
    "        src_ind = int(src_ind)\n",
    "        trg_ind = int(trg_ind)\n",
    "\n",
    "        if scores[i] < min_threshold:\n",
    "            break\n",
    "\n",
    "        if src_ind not in seen_src and trg_ind not in seen_trg:\n",
    "            seen_src.add(src_ind)\n",
    "            seen_trg.add(trg_ind)\n",
    "            fOut.write(\"{:.4f}\\t{}\\t{}\\n\".format(scores[i], source_sentences[src_ind].replace(\"\\t\", \" \"), target_sentences[trg_ind].replace(\"\\t\", \" \")))\n",
    "            sentences_written += 1\n",
    "\n",
    "print(\"Done. {} sentences written\".format(sentences_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "\n",
    "#First, we load the respective CLIP model\n",
    "# model = SentenceTransformer('clip-ViT-B-32')\n",
    "# Here we load the multilingual CLIP model. Note, this model can only encode text.\n",
    "# If you need embeddings for images, you must load the 'clip-ViT-B-32' model\n",
    "model = SentenceTransformer('clip-ViT-B-32-multilingual-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get about 25k images from Unsplash \n",
    "img_folder = 'photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
    "        \n",
    "    #Extract all images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to compute the embeddings\n",
    "# To speed things up, we destribute pre-computed embeddings\n",
    "# Otherwise you can also encode the images yourself.\n",
    "# To encode an image, you can use the following code:\n",
    "# from PIL import Image\n",
    "# img_emb = model.encode(Image.open(filepath))\n",
    "\n",
    "use_precomputed_embeddings = True\n",
    "\n",
    "if use_precomputed_embeddings: \n",
    "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
    "    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
    "        \n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_emb = pickle.load(fIn)  \n",
    "    print(\"Images:\", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n",
    "    print(\"Images:\", len(img_names))\n",
    "    img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a search function.\n",
    "def search(query, k=3):\n",
    "    # First, we encode the query (which can either be an image or a text string)\n",
    "    query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    # Then, we use the util.semantic_search function, which computes the cosine-similarity\n",
    "    # between the query embedding and all image embeddings.\n",
    "    # It then returns the top_k highest ranked images, which we output\n",
    "    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]\n",
    "    \n",
    "    print(\"Query:\")\n",
    "    display(query)\n",
    "    for hit in hits:\n",
    "        print(img_names[hit['corpus_id']])\n",
    "        display(IPImage(os.path.join(img_folder, img_names[hit['corpus_id']]), width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"Two dogs playing in the snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"고양이\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "   InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "print(train_examples)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import evaluation\n",
    "sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']\n",
    "sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
    "scores = [0.3, 0.6, 0.2]\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "# ... Your other code to load training data\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator, evaluation_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an example how to train SentenceTransformers in a multi-task setup.\n",
    "The system trains BERT on the AllNLI and on the STSbenchmark dataset.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Read the dataset\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 16\n",
    "model_save_path = 'output/training_multi-task_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_nli_samples = []\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'train':\n",
    "            label_id = label2int[row['label']]\n",
    "            train_nli_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "\n",
    "\n",
    "train_dataloader_nli = DataLoader(train_nli_samples, shuffle=True, batch_size=batch_size)\n",
    "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
    "\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "train_sts_samples = []\n",
    "dev_sts_samples = []\n",
    "test_sts_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n",
    "        inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n",
    "\n",
    "        if row['split'] == 'dev':\n",
    "            dev_sts_samples.append(inp_example)\n",
    "        elif row['split'] == 'test':\n",
    "            test_sts_samples.append(inp_example)\n",
    "        else:\n",
    "            train_sts_samples.append(inp_example)\n",
    "\n",
    "\n",
    "train_dataloader_sts = DataLoader(train_sts_samples, shuffle=True, batch_size=batch_size)\n",
    "train_loss_sts = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_sts_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training\n",
    "num_epochs = 4\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader_sts) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Here we define the two train objectives: train_dataloader_nli with train_loss_nli (i.e., SoftmaxLoss for NLI data)\n",
    "# and train_dataloader_sts with train_loss_sts (i.e., CosineSimilarityLoss for STSbenchmark data)\n",
    "# You can pass as many (dataloader, loss) tuples as you like. They are iterated in a round-robin way.\n",
    "train_objectives = [(train_dataloader_nli, train_loss_nli), (train_dataloader_sts, train_loss_sts)]\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=train_objectives,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_sts_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains an example how to extend an existent sentence embedding model to new languages.\n",
    "Given a (monolingual) teacher model you would like to extend to new languages, which is specified in the teacher_model_name\n",
    "variable. We train a multilingual student model to imitate the teacher model (variable student_model_name)\n",
    "on multiple languages.\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "This scripts downloads automatically the TED2020 corpus: https://github.com/UKPLab/sentence-transformers/blob/master/docs/datasets/TED2020.md\n",
    "This corpus contains transcripts from\n",
    "TED and TEDx talks, translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 16               #Batch size for training\n",
    "inference_batch_size = 16           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 1                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "# target_languages = set(['de', 'es', 'it', 'fr', 'ar', 'tr'])    # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "target_languages = set(['ko']) \n",
    "\n",
    "\n",
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "\n",
    "# Here we define train test and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])\n",
    "\n",
    "\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()\n",
    "\n",
    "\n",
    "\n",
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "\n",
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "sts_data = {}\n",
    "\n",
    "#Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename]['sentences1'].append(sent1)\n",
    "                    sts_data[filename]['sentences2'].append(sent2)\n",
    "                    sts_data[filename]['scores'].append(score)\n",
    "\n",
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "    evaluators.append(test_evaluator)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=num_warmup_steps,\n",
    "          evaluation_steps=num_evaluation_steps,\n",
    "          output_path=output_path,\n",
    "          save_best_model=True,\n",
    "          optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains an example how to make a SentenceTransformer model faster and lighter.\n",
    "This is achieved by using Knowledge Distillation: We use a well working teacher model to train\n",
    "a fast and light student model. The student model learns to imitate the produced\n",
    "sentence embeddings from the teacher. We train this on a diverse set of sentences we got\n",
    "from SNLI + Multi+NLI + Wikipedia.\n",
    "After the distillation is finished, the student model produce nearly the same embeddings as the\n",
    "teacher, however, it will be much faster.\n",
    "The script implements to options two options to initialize the student:\n",
    "Option 1: Train a light transformer model like TinyBERT to imitate the teacher\n",
    "Option 2: We take the teacher model and keep only certain layers, for example, only 4 layers.\n",
    "Option 2) works usually better, as we keep most of the weights from the teacher. In Option 1, we have to tune all\n",
    "weights in the student from scratch.\n",
    "There is a performance - speed trade-off. However, we found that a student with 4 instead of 12 layers keeps about 99.4%\n",
    "of the teacher performance, while being 2.3 times faster.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, evaluation\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "\n",
    "# Teacher Model: Model we want to distill to a smaller model\n",
    "teacher_model_name = 'stsb-roberta-base-v2'\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "output_path = \"output/model-distillation-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "use_layer_reduction = True\n",
    "\n",
    "#There are two options to create a light and fast student model:\n",
    "if use_layer_reduction:\n",
    "    # 1) Create a smaller student model by using only some of the teacher layers\n",
    "    student_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "    # Get the transformer model\n",
    "    auto_model = student_model._first_module().auto_model\n",
    "\n",
    "    # Which layers to keep from the teacher model. We equally spread the layers to keep over the original teacher\n",
    "    layers_to_keep = [5]\n",
    "    # layers_to_keep = [3, 7]\n",
    "    #layers_to_keep = [3, 7, 11]\n",
    "    # layers_to_keep = [1, 4, 7, 10]          #Keep 4 layers from the teacher\n",
    "    #layers_to_keep = [0, 2, 4, 6, 8, 10]\n",
    "    #layers_to_keep = [0, 1, 3, 4, 6, 7, 9, 10]\n",
    "\n",
    "    logging.info(\"Remove layers from student. Only keep these layers: {}\".format(layers_to_keep))\n",
    "    new_layers = torch.nn.ModuleList([layer_module for i, layer_module in enumerate(auto_model.encoder.layer) if i in layers_to_keep])\n",
    "    auto_model.encoder.layer = new_layers\n",
    "    auto_model.config.num_hidden_layers = len(layers_to_keep)\n",
    "else:\n",
    "    # 2) The other option is to train a small model like TinyBERT to imitate the teacher.\n",
    "    # You can find some small BERT models here: https://huggingface.co/nreimers\n",
    "    word_embedding_model = models.Transformer('nreimers/TinyBERT_L-4_H-312_v2')\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "\n",
    "inference_batch_size = 64\n",
    "train_batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "#We use AllNLI as a source of sentences for the distillation\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "#Further, we use sentences extracted from the English Wikipedia to train the distillation\n",
    "wikipedia_dataset_path = 'datasets/wikipedia-en-sentences.txt.gz'\n",
    "\n",
    "#We use the STS benchmark dataset to see how much performance we loose\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "\n",
    "#Download datasets if needed\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(wikipedia_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/wikipedia-en-sentences.txt.gz', wikipedia_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "#We need sentences to train our distillation. Here, we use sentences from AllNLI and from WikiPedia\n",
    "train_sentences_nli = set()\n",
    "dev_sentences_nli = set()\n",
    "\n",
    "train_sentences_wikipedia = []\n",
    "dev_sentences_wikipedia = []\n",
    "\n",
    "# Read ALLNLI\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'dev':\n",
    "            dev_sentences_nli.add(row['sentence1'])\n",
    "            dev_sentences_nli.add(row['sentence2'])\n",
    "        else:\n",
    "            train_sentences_nli.add(row['sentence1'])\n",
    "            train_sentences_nli.add(row['sentence2'])\n",
    "\n",
    "train_sentences_nli = list(train_sentences_nli)\n",
    "random.shuffle(train_sentences_nli)\n",
    "\n",
    "dev_sentences_nli = list(dev_sentences_nli)\n",
    "random.shuffle(dev_sentences_nli)\n",
    "dev_sentences_nli = dev_sentences_nli[0:5000] #Limit dev sentences to 5k\n",
    "\n",
    "# Read Wikipedia sentences file\n",
    "with gzip.open(wikipedia_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    wikipeda_sentences = [line.strip() for line in fIn]\n",
    "\n",
    "dev_sentences_wikipedia = wikipeda_sentences[0:5000] #Use the first 5k sentences from the wikipedia file for development\n",
    "train_sentences_wikipedia = wikipeda_sentences[5000:]\n",
    "\n",
    "\n",
    "# We use the STS benchmark dataset to measure the performance of student model im comparison to the teacher model\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'dev':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "dev_evaluator_sts = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "logging.info(\"Teacher Performance:\")\n",
    "dev_evaluator_sts(teacher_model)\n",
    "\n",
    "# Student model has fewer dimensions. Compute PCA for the teacher to reduce the dimensions\n",
    "if student_model.get_sentence_embedding_dimension() < teacher_model.get_sentence_embedding_dimension():\n",
    "    logging.info(\"Student model has fewer dimensions than the teacher. Compute PCA for down projection\")\n",
    "    pca_sentences = train_sentences_nli[0:20000] + train_sentences_wikipedia[0:20000]\n",
    "    pca_embeddings = teacher_model.encode(pca_sentences, convert_to_numpy=True)\n",
    "    pca = PCA(n_components=student_model.get_sentence_embedding_dimension())\n",
    "    pca.fit(pca_embeddings)\n",
    "\n",
    "    #Add Dense layer to teacher that projects the embeddings down to the student embedding size\n",
    "    dense = models.Dense(in_features=teacher_model.get_sentence_embedding_dimension(), out_features=student_model.get_sentence_embedding_dimension(), bias=False, activation_function=torch.nn.Identity())\n",
    "    dense.linear.weight = torch.nn.Parameter(torch.tensor(pca.components_))\n",
    "    teacher_model.add_module('dense', dense)\n",
    "\n",
    "    logging.info(\"Teacher Performance with {} dimensions:\".format(teacher_model.get_sentence_embedding_dimension()))\n",
    "    dev_evaluator_sts(teacher_model)\n",
    "\n",
    "\n",
    "\n",
    "# We train the student_model such that it creates sentence embeddings similar to the embeddings from the teacher_model\n",
    "# For this, we need a large set of sentences. These sentences are embedded using the teacher model,\n",
    "# and the student tries to mimic these embeddings. It is the same approach as used in: https://arxiv.org/abs/2004.09813\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=False)\n",
    "train_data.add_dataset([[sent] for sent in train_sentences_nli], max_sentence_length=256)\n",
    "train_data.add_dataset([[sent] for sent in train_sentences_wikipedia], max_sentence_length=256)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "# We create an evaluator, that measure the Mean Squared Error (MSE) between the teacher and the student embeddings\n",
    "dev_sentences = dev_sentences_nli + dev_sentences_wikipedia\n",
    "dev_evaluator_mse = evaluation.MSEEvaluator(dev_sentences, dev_sentences, teacher_model=teacher_model)\n",
    "\n",
    "# Train the student model to imitate the teacher\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "                  evaluator=evaluation.SequentialEvaluator([dev_evaluator_sts, dev_evaluator_mse]),\n",
    "                  epochs=1,\n",
    "                  warmup_steps=1000,\n",
    "                  evaluation_steps=5000,\n",
    "                  output_path=output_path,\n",
    "                  save_best_model=True,\n",
    "                  optimizer_params={'lr': 1e-4, 'eps': 1e-6, 'correct_bias': False},\n",
    "                  use_amp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cross-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This examples trains a CrossEncoder for the NLI task. A CrossEncoder takes a sentence pair\n",
    "as input and outputs a label. Here, it learns to predict the labels: \"contradiction\": 0, \"entailment\": 1, \"neutral\": 2.\n",
    "It does NOT produce a sentence embedding and does NOT work for individual sentences.\n",
    "Usage:\n",
    "python training_nli.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "#### /print debug information to stdout\n",
    "\n",
    "\n",
    "#As dataset, we use SNLI + MultiNLI\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logger.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        label_id = label2int[row['label']]\n",
    "        if row['split'] == 'train':\n",
    "            train_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "        else:\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_allnli-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "#Define our CrossEncoder model. We use distilroberta-base as basis and setup it up to predict 3 labels\n",
    "model = CrossEncoder('distilroberta-base', num_labels=len(label2int))\n",
    "\n",
    "#We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "#During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
    "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples, name='AllNLI-dev')\n",
    "\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=10000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea47873212ba17024efc01a8d4a5d2c9efad0164b5bd42755e9d5fa44324ed6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
