{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references: \n",
    "https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9\n",
    "\n",
    "https://www.sbert.net/examples/training/multilingual/README.html\n",
    "\n",
    "This notebook trains a cross language model with knowledge distillation, where the teacher model is a pre-trained model on big English dataset, and the student model is finetuned/trained from scratch with pairs of English-Korean data. To solve that, we can use the model architecture similar with Siamese and Triplet network structures to extend SBERT to new language. The idea is simple, first we produces sentence embeddings in English sentence by SBERT, we call Teacher model. Then we create new model for our desired language, we call Student model, and this model tries to mimic the Teacher model. In other word, the original English sentence will be trained in Student model in order to get the vector same as one in Teacher model.\n",
    "\n",
    "![alt text](img/student-korean.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "# import zipfile\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 32              #Batch size for training\n",
    "inference_batch_size = 32           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['ko'])    # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "\n",
    "This scripts downloads automatically the TED2020 corpus: https://github.com/UKPLab/sentence-transformers/blob/master/docs/datasets/TED2020.md\n",
    "This corpus contains transcripts from TED and TEDx talks, translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained teacher model and prepare student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset into torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write evaluate funtion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "# all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "# sts_data = {}\n",
    "\n",
    "# #Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "# with zipfile.ZipFile(sts_corpus) as zip:\n",
    "#     filelist = zip.namelist()\n",
    "#     sts_files = []\n",
    "\n",
    "#     for i in range(len(all_languages)):\n",
    "#         for j in range(i, len(all_languages)):\n",
    "#             lang1 = all_languages[i]\n",
    "#             lang2 = all_languages[j]\n",
    "#             filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "#             if filepath not in filelist:\n",
    "#                 lang1, lang2 = lang2, lang1\n",
    "#                 filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "\n",
    "#             if filepath in filelist:\n",
    "#                 filename = os.path.basename(filepath)\n",
    "#                 sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "#                 fIn = zip.open(filepath)\n",
    "#                 for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "#                     sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "#                     score = float(score)\n",
    "#                     sts_data[filename]['sentences1'].append(sent1)\n",
    "#                     sts_data[filename]['sentences2'].append(sent2)\n",
    "#                     sts_data[filename]['scores'].append(score)\n",
    "\n",
    "# for filename, data in sts_data.items():\n",
    "#     test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "#     evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "#           evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "#           epochs=num_epochs,\n",
    "#           warmup_steps=num_warmup_steps,\n",
    "#           evaluation_steps=num_evaluation_steps,\n",
    "#           output_path=output_path,\n",
    "#           save_best_model=True,\n",
    "#           optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the model with semantic text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\"korsts\")\n",
    "corpus_reader = Korpora.load(\"korsts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "\n",
    "student_model = SentenceTransformer('output/make-multilingual-en-ko-2022-02-07_13-22-03/')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = list(corpus_reader.dev.get_all_texts())\n",
    "corpus_embeddings = student_model.encode(corpus)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', \n",
    "        'Someone in a gorilla costume is playing a set of drums.', \n",
    "        '오늘 날씨가 좋다.',\n",
    "        '지금 몇 시지?']\n",
    "query_embeddings = student_model.encode(queries)\n",
    "\n",
    "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
    "closest_n = 3\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 3 most similar sentences in corpus:\\n\")\n",
    "\n",
    "    for idx, distance in results[0:closest_n]:\n",
    "        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea47873212ba17024efc01a8d4a5d2c9efad0164b5bd42755e9d5fa44324ed6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
