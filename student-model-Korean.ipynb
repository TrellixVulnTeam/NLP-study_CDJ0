{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references: \n",
    "https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9\n",
    "\n",
    "https://www.sbert.net/examples/training/multilingual/README.html\n",
    "\n",
    "This notebook trains a cross language model with knowledge distillation, where the teacher model is a pre-trained model on big English dataset, and the student model is finetuned/trained from scratch with pairs of English-Korean data. To solve that, we can use the model architecture similar with Siamese and Triplet network structures to extend SBERT to new language. The idea is simple, first we produces sentence embeddings in English sentence by SBERT, we call Teacher model. Then we create new model for our desired language, we call Student model, and this model tries to mimic the Teacher model. In other word, the original English sentence will be trained in Student model in order to get the vector same as one in Teacher model.\n",
    "\n",
    "![alt text](img/student-korean.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "# import zipfile\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 32              #Batch size for training\n",
    "inference_batch_size = 32           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['ko'])    # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "\n",
    "This scripts downloads automatically the TED2020 corpus: https://github.com/UKPLab/sentence-transformers/blob/master/docs/datasets/TED2020.md\n",
    "This corpus contains transcripts from TED and TEDx talks, translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained teacher model and prepare student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:10:56 - Load teacher model\n",
      "2022-02-08 11:10:56 - Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n",
      "2022-02-08 11:11:09 - Use pytorch device: cuda\n",
      "2022-02-08 11:11:09 - Create student model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:11:21 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset into torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:11:22 - Load parallel-sentences/TED2020-en-ko-train.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write evaluate funtion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:11:25 - Create evaluator for parallel-sentences/TED2020-en-ko-dev.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "# all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "# sts_data = {}\n",
    "\n",
    "# #Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "# with zipfile.ZipFile(sts_corpus) as zip:\n",
    "#     filelist = zip.namelist()\n",
    "#     sts_files = []\n",
    "\n",
    "#     for i in range(len(all_languages)):\n",
    "#         for j in range(i, len(all_languages)):\n",
    "#             lang1 = all_languages[i]\n",
    "#             lang2 = all_languages[j]\n",
    "#             filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "#             if filepath not in filelist:\n",
    "#                 lang1, lang2 = lang2, lang1\n",
    "#                 filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "\n",
    "#             if filepath in filelist:\n",
    "#                 filename = os.path.basename(filepath)\n",
    "#                 sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "#                 fIn = zip.open(filepath)\n",
    "#                 for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "#                     sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "#                     score = float(score)\n",
    "#                     sts_data[filename]['sentences1'].append(sent1)\n",
    "#                     sts_data[filename]['sentences2'].append(sent2)\n",
    "#                     sts_data[filename]['scores'].append(score)\n",
    "\n",
    "# for filename, data in sts_data.items():\n",
    "#     test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "#     evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "#           evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "#           epochs=num_epochs,\n",
    "#           warmup_steps=num_warmup_steps,\n",
    "#           evaluation_steps=num_evaluation_steps,\n",
    "#           output_path=output_path,\n",
    "#           save_best_model=True,\n",
    "#           optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the model with semantic text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-train.tsv\n",
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-dev.tsv\n",
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-test.tsv\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : KakaoBrain\n",
      "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
      "    References :\n",
      "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
      "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
      "           (https://arxiv.org/abs/2004.03289)\n",
      "\n",
      "    This is the dataset repository for our paper\n",
      "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
      "    (https://arxiv.org/abs/2004.03289)\n",
      "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
      "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-train.tsv\n",
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-dev.tsv\n",
      "[Korpora] Corpus `korsts` is already installed at /home/vips/Korpora/korsts/sts-test.tsv\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\"korsts\")\n",
    "corpus_reader = Korpora.load(\"korsts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:17:45 - Load pretrained SentenceTransformer: output/make-multilingual-en-ko-2022-02-07_13-22-03/\n",
      "2022-02-08 11:17:47 - Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15d7f40f2a341fabce9624de2ea2649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19852ff426949f4988c4afea1b98d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "\n",
      "Query: A man is eating pasta.\n",
      "\n",
      "Top 3 most similar sentences in corpus:\n",
      "\n",
      "한 남자가 피자를 먹고 있다. (Score: 0.8637)\n",
      "한 남자가 음식을 먹고 있다. (Score: 0.8078)\n",
      "한 남자가 피자에 치즈를 넣는다. (Score: 0.7777)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: Someone in a gorilla costume is playing a set of drums.\n",
      "\n",
      "Top 3 most similar sentences in corpus:\n",
      "\n",
      "한 남자가 드럼을 치고 있다. (Score: 0.7164)\n",
      "한 남자가 어쿠스틱 기타를 연주한다. (Score: 0.6845)\n",
      "한 남자가 기타를 연주한다. (Score: 0.6718)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: 오늘 날씨가 좋다.\n",
      "\n",
      "Top 3 most similar sentences in corpus:\n",
      "\n",
      "이것은 지나치게 명백할 수도 있지만, 미국 영어에서는 \"환영합니다\"라고 말하는 것이 확실히 예의 바르고 표준적입니다. (Score: 0.4350)\n",
      "네가 하키를 시작했다는 말을 들으니 기쁘다, 그것은 대단한 스포츠 야! (Score: 0.4212)\n",
      "해 질 녘에 해안을 따라 걷는 두 사람. (Score: 0.4154)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: 지금 몇 시지?\n",
      "\n",
      "Top 3 most similar sentences in corpus:\n",
      "\n",
      "4월 23일 역사에서 오늘 (Score: 0.5966)\n",
      "금요일, 그 콩코드는 일출을 전후하여 시작되었고 떠오르는 태양에서 바로 발사되는 것처럼 보였다. (Score: 0.4652)\n",
      "그 시간은 3월 18일 오전 4시경, 첫 번째 핀포인트 미사일이 수도에 쏟아져 내리기 불과 몇 시간 전이었다. (Score: 0.4493)\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "\n",
    "student_model = SentenceTransformer('output/make-multilingual-en-ko-2022-02-07_13-22-03/')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = list(corpus_reader.dev.get_all_texts())\n",
    "corpus_embeddings = student_model.encode(corpus)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', \n",
    "        'Someone in a gorilla costume is playing a set of drums.', \n",
    "        '오늘 날씨가 좋다.',\n",
    "        '지금 몇 시지?']\n",
    "query_embeddings = student_model.encode(queries)\n",
    "\n",
    "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
    "closest_n = 3\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 3 most similar sentences in corpus:\\n\")\n",
    "\n",
    "    for idx, distance in results[0:closest_n]:\n",
    "        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ea47873212ba17024efc01a8d4a5d2c9efad0164b5bd42755e9d5fa44324ed6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
